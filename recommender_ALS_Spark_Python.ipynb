{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Recommender System using ALS Algorithm with Apache Spark and Python\n",
    "+ **Estimated Execution Time (whole script): 2 minutes**\n",
    "+ **Estimated Time (to complete the project): 8 hours**\n",
    "\n",
    "## Description\n",
    "\n",
    "For this project, you are to create a recommender system that will recommend new musical artists to a user based on their listening history. Suggesting different songs or musical artists to a user is important to many music streaming services, such as Pandora and Spotify. In addition, this type of recommender system could also be used as a means of suggesting TV shows or movies to a user (e.g., Netflix). \n",
    "\n",
    "To create this system you will be using Spark and the collaborative filtering technique. The instructions for completing this project will be laid out entirely in this file. You will have to implement any missing code as well as answer any questions.\n",
    "\n",
    "**Submission Instructions:** \n",
    "* Add all of your updates to this Jupyter Notebook file and do NOT clear any of the output you get from running your code.\n",
    "* Upload this file and the genererated HTML onto Moodle as a single zip folder called with your user name.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "You will be using some publicly available song data from audioscrobbler, which can be found [here](http://www-etud.iro.umontreal.ca/~bergstrj/audioscrobbler_data.html). However, we modified the original data files so that the code will run in a reasonable time on a single machine. The reduced data files have been suffixed with `_small.txt` and contains only the information relevant to the top 50 most prolific users (highest artist play counts).\n",
    "\n",
    "The original data file `user_artist_data.txt` contained about 141,000 unique users, and 1.6 million unique artists. About 24.2 million usersâ€™ plays of artists are recorded, along with their count.\n",
    "\n",
    "Note that when plays are scribbled, the client application submits the name of the artist being played. This name could be misspelled or nonstandard, and this may only be detected later. For example, \"The Smiths\", \"Smiths, The\", and \"the smiths\" may appear as distinct artist IDs in the data set, even though they clearly refer to the same artist. So, the data set includes `artist_alias.txt`, which maps artist IDs that are known misspellings or variants to the canonical ID of that artist.\n",
    "\n",
    "The `artist_data.txt` file then provides a map from the canonical artist ID to the name of the artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.mllib.recommendation import *\n",
    "import random\n",
    "from operator import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f2fd34f72120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# YOUR CODE GOES HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"recommendation-system\"\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# creating spark context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_launch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36m_launch_gateway\u001b[1;34m(conf, insecure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Context\n",
    "# YOUR CODE GOES HERE\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\" , \"recommendation-system\")          # creating spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "#Load the three datasets into RDDs and name them `artistData`, `artistAlias`, and `userArtistData`. View the README, or the files themselves, to see how this data is formated. Some of the files have tab delimeters while some have space delimiters. Make sure that your `userArtistData` RDD contains only the canonical artist IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c9e85d415873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import test files from location into RDD variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# YOUR CODE GOES HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0martistData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data_raw/artist_data_small.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m#reading and parsing artistDataFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0martistAlias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data_raw/artist_alias_small.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m#reading and parsing artistAliasDataFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0muserArtistData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data_raw/user_artist_data_small.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#reading and parsing userAliasDataFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# Import test files from location into RDD variables\n",
    "# YOUR CODE GOES HERE \n",
    "artistData = sc.textFile(\"./data_raw/artist_data_small.txt\").map(lambda strings: strings.split('\\t'))           #reading and parsing artistDataFile\n",
    "artistAlias = sc.textFile(\"./data_raw/artist_alias_small.txt\").map(lambda strings: strings.split('\\t'))         #reading and parsing artistAliasDataFile\n",
    "userArtistData = sc.textFile(\"./data_raw/user_artist_data_small.txt\").map(lambda strings: strings.split(' '))   #reading and parsing userAliasDataFile\n",
    "    \n",
    "artistAliasMapValue = artistAlias.collectAsMap()           #creates map of artist RDD\n",
    "def cononicalId(userdata):                                 #function to return the canonical id of artists\n",
    "    if userdata in artistAliasMapValue:\n",
    "        return artistAliasMapValue[userdata]\n",
    "    else:\n",
    "        return userdata\n",
    "    \n",
    "userArtistData = userArtistData.map(lambda userdata: (userdata[0],cononicalId(userdata[1]),userdata[2]))    #return the user artitst data with the canonical id repalced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "In the blank below, write some code that with find the users' total play counts. Find the three users with the highest number of total play counts (sum of all counters) and print the user ID, the total play count, and the mean play count (average number of times a user played an artist). Your output should look as follows:\n",
    "```\n",
    "User 1059637 has a total play count of 674412 and a mean play count of 1878.\n",
    "User 2064012 has a total play count of 548427 and a mean play count of 9455.\n",
    "User 2069337 has a total play count of 393515 and a mean play count of 1519.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a sequence into seperate entities and store as int\n",
    "# YOUR CODE GOES HERE\n",
    "userArtistData = userArtistData.map(lambda value:(int(value[0]),int(value[1]),int(value[2]))) #converting userArtistdata into int\n",
    "\n",
    "# Create a dictionary of the 'artistAlias' dataset\n",
    "# YOUR CODE GOES HERE\n",
    "artistAliasDictValue = {}\n",
    "for line in artistAlias.collect():                       #storing as a dictionary the values of artistAlias\n",
    "    key = line[0]\n",
    "    val = line[1]\n",
    "    artistAliasDictValue[key] = val    \n",
    "\n",
    "# If artistid exists, replace with artistsid from artistAlias, else retain original\n",
    "# YOUR CODE GOES HERE\n",
    "def cononicalId1(userdata):                             #function to return the canonical id of artists\n",
    "    if userdata in artistAliasDictValue: \n",
    "        return artistAliasDictValue[userdata]\n",
    "    else:\n",
    "        return userdata\n",
    "userArtistData = userArtistData.map(lambda userdata: (userdata[0],cononicalId1(userdata[1]),userdata[2]))\n",
    "\n",
    "\n",
    "# Create an RDD consisting of 'userid' and 'playcount' objects of original tuple\n",
    "# YOUR CODE GOES HERE\n",
    "userArtistDatatwocolumns = userArtistData.map(lambda userdata:(userdata[0],userdata[2]))   #creates RDD with 'userid' and 'playcount'\n",
    "\n",
    "# Count instances by key and store in broadcast variable\n",
    "# YOUR CODE GOES HERE\n",
    "playcountsforeachuser = sc.broadcast(userArtistDatatwocolumns.countByKey())              #broadcast variable for key counts\n",
    "\n",
    "# Compute and display users with the highest playcount along with their mean playcount across artists\n",
    "# YOUR CODE GOES HERE\n",
    "sortedByPlayCount=userArtistDatatwocolumns.reduceByKey(lambda x,y: x+y)     #stores values of playcounts as a sum\n",
    "firstThree = sortedByPlayCount.take(3)                                      #takes the first three values\n",
    "for i in firstThree:\n",
    "    print(\"User \" + str(i[0]) + \" has a total play count of  \" + str(i[1]) + \" and a mean play count of \"+ str(int(i[1]/playcountsforeachuser.value[i[0]])) +\".\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  Splitting Data for Testing\n",
    "\n",
    "Use the [randomSplit](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) function to divide the data (`userArtistData`) into:\n",
    "* A training set, `trainData`, that will be used to train the model. This set should constitute 40% of the data.\n",
    "* A validation set, `validationData`, used to perform parameter tuning. This set should constitute 40% of the data.\n",
    "* A test set, `testData`, used for a final evaluation of the model. This set should constitute 20% of the data.\n",
    "\n",
    "Use a random seed value of 13. Since these datasets will be repeatedly used you will probably want to persist them in memory using the [cache](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache) function.\n",
    "\n",
    "In addition, print out the first 3 elements of each set as well as their sizes; if you created these sets correctly, your output should look like the following:\n",
    "```\n",
    "[(1059637, 1000049, 1), (1059637, 1000056, 1), (1059637, 1000114, 2)]\n",
    "[(1059637, 1000010, 238), (1059637, 1000062, 11), (1059637, 1000123, 2)]\n",
    "[(1059637, 1000094, 1), (1059637, 1000112, 423), (1059637, 1000113, 5)]\n",
    "19761\n",
    "19862\n",
    "9858\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'userArtistData' dataset into training, validation and test datasets. Store in cache for frequent access\n",
    "# YOUR CODE GOES HERE\n",
    "trainData, validationData, testData = userArtistData.randomSplit((40,40,20),seed=13) #splits data into three set with seed = 13\n",
    "trainData.cache()                                                                    #performs caches to persist in memory\n",
    "validationData.cache()\n",
    "testData.cache()\n",
    "\n",
    "print(trainData.take(3))                                                            #returns the first 3\n",
    "print(validationData.take(3))\n",
    "print(testData.take(3))\n",
    "print(trainData.count())                                                            #returns count\n",
    "print(validationData.count())\n",
    "print(testData.count())\n",
    "# Display the first 3 records of each dataset followed by the total count of records for each datasets\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Recommender Model\n",
    "\n",
    "For this project, we will train the model with implicit feedback. You can read more information about this from the collaborative filtering page: [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html). The [function you will be using](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS.trainImplicit) has a few tunable parameters that will affect how the model is built. Therefore, to get the best model, we will do a small parameter sweep and choose the model that performs the best on the validation set\n",
    "\n",
    "Therefore, we must first devise a way to evaluate models. Once we have a method for evaluation, we can run a parameter sweep, evaluate each combination of parameters on the validation data, and choose the optimal set of parameters. The parameters then can be used to make predictions on the test data.\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "Although there may be several ways to evaluate a model, we will use a simple method here. Suppose we have a model and some dataset of *true* artist plays for a set of users. This model can be used to predict the top X artist recommendations for a user and these recommendations can be compared the artists that the user actually listened to (here, X will be the number of artists in the dataset of *true* artist plays). Then, the fraction of overlap between the top X predictions of the model and the X artists that the user actually listened to can be calculated. This process can be repeated for all users and an average value returned.\n",
    "\n",
    "For example, suppose a model predicted [1,2,4,8] as the top X=4 artists for a user. Suppose, that user actually listened to the artists [1,3,7,8]. Then, for this user, the model would have a score of 2/4=0.5. To get the overall score, this would be performed for all users, with the average returned.\n",
    "\n",
    "**NOTE: when using the model to predict the top-X artists for a user, do not include the artists listed with that user in the training data.**\n",
    "\n",
    "Name your function `modelEval` and have it take a model (the output of ALS.trainImplicit) and a dataset as input. For parameter tuning, the dataset parameter should be set to the validation data (`validationData`). After parameter tuning, the model can be evaluated on the test data (`testData`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEval(model, dataset):\n",
    "    \n",
    "    # All artists in the 'userArtistData' dataset\n",
    "    # YOUR CODE GOES HERE\n",
    "    artistEvery = list(userArtistData.map(lambda i: i[1]).collect())     #list of all artists\n",
    "    \n",
    "    # Set of all users in the current (Validation/Testing) dataset\n",
    "    # YOUR CODE GOES HERE\n",
    "    userAll = set(dataset.map(lambda i: i[0]).collect())    #set of all users\n",
    "    \n",
    "    \n",
    "    # Create a dictionary of (key, values) for current (Validation/Testing) dataset\n",
    "    # YOUR CODE GOES HERE\n",
    "    artistUserDataDict = {}\n",
    "    for line in dataset.collect():  \n",
    "        key = line[0]\n",
    "        val = line[1]\n",
    "        if key in artistUserDataDict.keys():\n",
    "            artistUserDataDict[key].append(val)\n",
    "        else:\n",
    "            artistUserDataDict[key] = [val]    \n",
    "                    \n",
    "\n",
    "\n",
    "    # Create a dictionary of (key, values) for training dataset\n",
    "    # YOUR CODE GOES HERE\n",
    "    artistUserTrainDict = {}\n",
    "    for line in trainData.collect():  \n",
    "        key = line[0]\n",
    "        val = line[1]\n",
    "        if key in artistUserTrainDict.keys():\n",
    "            artistUserTrainDict[key].append(val)\n",
    "        else:\n",
    "            artistUserTrainDict[key] = [val]    \n",
    "\n",
    "    # For each user, calculate the prediction score i.e. similarity between predicted and actual artists\n",
    "    # YOUR CODE GOES HERE\n",
    "    finalScore = 0.0\n",
    "    def maketuple(userdata,artistdata):                             #function to return the tuple values\n",
    "        return (userdata,artistdata)\n",
    "    for user in artistUserDataDict:\n",
    "            actualArtistValues = artistUserDataDict[user]           #returns actual artist values\n",
    "            countofActualArtists = len(actualArtistValues)\n",
    "            setartistEvery = set(artistEvery)\n",
    "            setArtistUserinTrain = set(artistUserTrainDict[user])\n",
    "            artistsNotInTrainData = []\n",
    "            for artist in setartistEvery:\n",
    "                if artist not in setArtistUserinTrain:\n",
    "                    artistsNotInTrainData.append(artist)                         #finds artists not in train data\n",
    "            userArtistPair = []\n",
    "            for artist in artistsNotInTrainData:                                   #make user artsts pairs\n",
    "                userArtistPair.append(maketuple(user,artist)) \n",
    "            userArtistPairRDD = sc.parallelize(userArtistPair)\n",
    "            valueReturned=model.predictAll(userArtistPairRDD).sortBy(lambda x: x[2],ascending=False)      #predicts values and sorts\n",
    "            predictionsTopvals = valueReturned.take(countofActualArtists)\n",
    "            #PredictionValuesMap = valueReturned.map(lambda x: x[1])\n",
    "            PredictionValue = []\n",
    "            for line in predictionsTopvals:\n",
    "                PredictionValue.append(line[1])\n",
    "            #PredictionValue = PredictionValuesMap.take(countofActualArtists)\n",
    "            setPrediction = set(PredictionValue)\n",
    "            setTrueArtist = set(actualArtistValues)\n",
    "            IntersectionValuesPresent = []\n",
    "            for line in setPrediction:\n",
    "                if line not in setTrueArtist:\n",
    "                    IntersectionValues.append(line)\n",
    "            IntersectionValues = set(IntersectionValuesPresent)       \n",
    "            #IntersectionValues = setPrediction.intersection(setTrueArtist)\n",
    "            lengthIntersection = len(IntersectionValues)\n",
    "            finalScore = finalScore + (float(lengthIntersection) / float(countofActualArtists))       #prints finalsccore\n",
    "        \n",
    "     # Print average score of the model for all users for the specified rank\n",
    "     # YOUR CODE GOES HERE\n",
    "    print(\"The model score for rank\" + \" \" + str(model.rank)+ \" \"+ \"is\" + \" \" + \"~\" + str(finalScore/len(artistUserDataDict)))                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction\n",
    "\n",
    "Now we can build the best model possibly using the validation set of data and the `modelEval` function. Although, there are a few parameters we could optimize, for the sake of time, we will just try a few different values for the [rank parameter](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html#collaborative-filtering) (leave everything else at its default value, **except make `seed`=345**). Loop through the values [2, 10, 20] and figure out which one produces the highest scored based on your model evaluation function.\n",
    "\n",
    "Note: this procedure may take several minutes to run.\n",
    "\n",
    "For each rank value, print out the output of the `modelEval` function for that model. Your output should look as follows:\n",
    "```\n",
    "The model score for rank 2 is ~0.090431\n",
    "The model score for rank 10 is ~0.095294\n",
    "The model score for rank 20 is ~0.090248\n",
    "```\n",
    "Step below takes 2 minutes to run. Uncomment to if you wish to run and calculate model score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rankList = [2,10,20]\n",
    "for rank in rankList:\n",
    "    model = ALS.trainImplicit(trainData, rank , seed=345)\n",
    "    modelEval(model,validationData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the bestModel, we will check the results over the test data. Your result should be ~`0.0507`.  \n",
    "Step below takes 1 minute to run. Uncomment last line if you wish to run and calculate model score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = ALS.trainImplicit(trainData, rank=10, seed=345)\n",
    "modelEval(bestModel, testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Some Artist Recommendations\n",
    "Using the best model above, predict the top 5 artists for user `1059637` using the [recommendProducts](http://spark.apache.org/docs/1.5.2/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.MatrixFactorizationModel.recommendProducts) function. Map the results (integer IDs) into the real artist name using `artistAlias`. Print the results. The output should look as follows:\n",
    "```\n",
    "Artist 0: My Chemical Romance\n",
    "Artist 1: Something Corporate\n",
    "Artist 2: Evanescence\n",
    "Artist 3: Alanis Morissette\n",
    "Artist 4: Counting Crows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 5 artists for a particular user and list their names\n",
    "# YOUR CODE GOES HERE\n",
    "highest5ratings = bestModel.recommendProducts(1059637, 5)                         #obtains the best model recommendations\n",
    "artistDataDictVal = {}\n",
    "for line in artistData.collect():                                                 #storing as a dictionary the values of artistData    \n",
    "    key = line[0]\n",
    "    val = line[1]\n",
    "    artistDataDictVal[key] = val                                        \n",
    "\n",
    "artistRecommend = []                                                              #adds top 5 artists into list\n",
    "for line in highest5ratings: \n",
    "    artistRecommend.append(line[1])\n",
    "count = 0 \n",
    "\n",
    "\n",
    "for line in artistRecommend:\n",
    "    print(\"Artist\" + \" \" + str(count) + \":\", artistDataDictVal[str(line)])\n",
    "    count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
